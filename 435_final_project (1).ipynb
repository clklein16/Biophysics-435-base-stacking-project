{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "435 final project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbCI_UdsuNv7",
        "colab_type": "text"
      },
      "source": [
        "# Carl Klein - Biophysics 435 Final Project: Predicting Base Stacking and SASA for RNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGPiVS-NOdug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import warnings\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import expon as sp_expon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al2GhuFEJyNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Global variable \n",
        "NUMBER_CHEMICAL_SHIFT_TYPE = 19\n",
        "\n",
        "def get_cs_all(cs_all, id = \"2KOC\"):\n",
        "  '''    \n",
        "    This function gets chemical shifts for a particular RNA. \n",
        "    Assumes each RNA has a unique id  \n",
        "  '''\n",
        "  return(cs_all[(cs_all.id == id)])\n",
        "\n",
        "def get_cs_residues(cs_i, resid, dummy = 0):\n",
        "  '''    \n",
        "    This function return an array containing the chemical shifts for a particular residues in an RNA.    \n",
        "  '''\n",
        "  cs_tmp = cs_i[(cs_i.resid == resid)].drop(['id', 'resid', 'resname', 'stacking', 'orientation', 'pseudoknot', 'base_pairing', 'sugar_puckering'], axis=1)\n",
        "  info_tmp = cs_i[(cs_i.resid == resid)]\n",
        "  if (cs_tmp.shape[0] != 1):\n",
        "     return(dummy*np.ones(shape=(1, NUMBER_CHEMICAL_SHIFT_TYPE)))\n",
        "  else:\n",
        "     return(cs_tmp.values)\n",
        "    \n",
        "def get_resnames(cs_i, resid, dummy = \"UNK\"):\n",
        "  '''    \n",
        "    This function returns the residue name for specified residue (resid)\n",
        "  '''\n",
        "  cs_tmp = cs_i[(cs_i.resid == resid)]  \n",
        "  if (cs_tmp.shape[0] != 1):\n",
        "     return(dummy)\n",
        "  else:\n",
        "     return(cs_tmp['resname'].values[0])\n",
        "\n",
        "def get_cs_features(cs_i, resid, neighbors=1):\n",
        "  '''    \n",
        "  This function chemical shifts and resnames for residue (resid) and its neighbors        \n",
        "\n",
        "  '''\n",
        "  cs = []\n",
        "  resnames = []\n",
        "  for i in range(resid-neighbors, resid+neighbors+1):\n",
        "    cs.append(get_cs_residues(cs_i, i))\n",
        "    resnames.append(get_resnames(cs_i, i))\n",
        "  return(resnames, np.array(cs))\n",
        "\n",
        "def get_columns_names(neighbors = 3, chemical_shift_types = 19):\n",
        "  '''\n",
        "    \n",
        "    Helper function that writes out the required column names\n",
        "    \n",
        "  '''\n",
        "\n",
        "  columns = ['id', 'resname', 'resid', 'stacking', 'orientation', 'pseudoknot', 'base_pairing', 'sugar_puckering']\n",
        "  for i in range(0, neighbors*chemical_shift_types):\n",
        "    columns.append(i)\n",
        "  return(columns)\n",
        "\n",
        "def write_out_resname(neighbors=1):\n",
        "  '''\n",
        "  \n",
        "    Helper function that writes out the column names associated resnames for a given residue and its neighbors\n",
        "    \n",
        "  '''  \n",
        "  colnames = []\n",
        "  for i in range(1-neighbors-1, neighbors+1):\n",
        "    if i < 0: \n",
        "      colnames.append('R%s'%i)\n",
        "    elif i > 0: \n",
        "      colnames.append('R+%s'%i)\n",
        "    else: \n",
        "      colnames.append('R')\n",
        "  return(colnames)    \n",
        "\n",
        "\n",
        "def get_cs_features_rna(cs, neighbors=1, retain = ['id', 'stacking', 'resid', 'orientation', 'pseudoknot', 'base_pairing', 'sugar_puckering']):\n",
        "  '''    \n",
        "    This function generates the complete required data frame an RNA    \n",
        "  '''\n",
        "  all_features = []\n",
        "  all_resnames = []\n",
        "  for resid in cs['resid'].unique():\n",
        "    resnames, features = get_cs_features(cs, resid, neighbors)\n",
        "    all_features.append(features.flatten())\n",
        "    all_resnames.append(resnames)\n",
        "\n",
        "  all_resnames = pd.DataFrame(all_resnames, dtype='object', columns = write_out_resname(neighbors))\n",
        "  all_features = pd.DataFrame(all_features, dtype='object')\n",
        "  info = pd.DataFrame(cs[retain].values, dtype='object', columns = retain)\n",
        "  return(pd.concat([info, all_resnames, all_features], axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYuL1KewSo3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start: your code\n",
        "def get_cs_features_rna_all(cs, neighbors = 2, retain = ['id', 'stacking', 'resid', 'orientation', 'pseudoknot', 'base_pairing', 'sugar_puckering']):\n",
        "  ids = cs['id'].unique()\n",
        "  for i,id in enumerate(ids):\n",
        "    if i == 0:\n",
        "      cs_new = get_cs_features_rna(get_cs_all(cs, id), neighbors)\n",
        "    else:\n",
        "      cs_new = cs_new.append(get_cs_features_rna(get_cs_all(cs, id), neighbors), sort = False)\n",
        "  \n",
        "  # End: your code\n",
        "  return(cs_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIxAxfOoWWeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get -qq install -y python-rdkit librdkit1 rdkit-data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DHsTwq2WfXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q joblib pandas sklearn tensorflow pillow deepchem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00yOiD0VWjkT",
        "colab_type": "code",
        "outputId": "efdaabda-2280-49f5-b89b-914a64b17b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMHKxB-pOvu6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get the dataset and store it in a Pandas Dataframe\n",
        "url=\"https://drive.google.com/uc?id=1e-SHtWDtg4mD_th3_4Jmq9r1iiQC32wT\"\n",
        "s=requests.get(url).content\n",
        "c=pd.read_csv(io.StringIO(s.decode('utf-8')), sep= \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_xqsMpofnvA",
        "colab_type": "code",
        "outputId": "7a6d2df1-31c0-4cbf-b31f-c10e7465e76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "#dataset prior to one-hot encoding\n",
        "cs_all = get_cs_features_rna_all(c, neighbors = 0)\n",
        "\n",
        "cs_all"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>stacking</th>\n",
              "      <th>resid</th>\n",
              "      <th>orientation</th>\n",
              "      <th>pseudoknot</th>\n",
              "      <th>base_pairing</th>\n",
              "      <th>sugar_puckering</th>\n",
              "      <th>R</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1A60</td>\n",
              "      <td>stack</td>\n",
              "      <td>1</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>0</td>\n",
              "      <td>92.7</td>\n",
              "      <td>74.9</td>\n",
              "      <td>72</td>\n",
              "      <td>82.5</td>\n",
              "      <td>65.4</td>\n",
              "      <td>153.6</td>\n",
              "      <td>109.8</td>\n",
              "      <td>157</td>\n",
              "      <td>142.1</td>\n",
              "      <td>5.65</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.23</td>\n",
              "      <td>7.469</td>\n",
              "      <td>6.124</td>\n",
              "      <td>4.07</td>\n",
              "      <td>3.99</td>\n",
              "      <td>8.413</td>\n",
              "      <td>8.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1A60</td>\n",
              "      <td>stack</td>\n",
              "      <td>2</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>1</td>\n",
              "      <td>91.9</td>\n",
              "      <td>75.7</td>\n",
              "      <td>72.4</td>\n",
              "      <td>82.3</td>\n",
              "      <td>66.1</td>\n",
              "      <td>155.207</td>\n",
              "      <td>118.377</td>\n",
              "      <td>160.7</td>\n",
              "      <td>141.4</td>\n",
              "      <td>5.88</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.31</td>\n",
              "      <td>7.83</td>\n",
              "      <td>5.549</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.15</td>\n",
              "      <td>7.871</td>\n",
              "      <td>7.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1A60</td>\n",
              "      <td>stack</td>\n",
              "      <td>3</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>2</td>\n",
              "      <td>92.2</td>\n",
              "      <td>76.1</td>\n",
              "      <td>72.3</td>\n",
              "      <td>84.7</td>\n",
              "      <td>66</td>\n",
              "      <td>154.079</td>\n",
              "      <td>120.289</td>\n",
              "      <td>160.7</td>\n",
              "      <td>141.2</td>\n",
              "      <td>5.77</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.09</td>\n",
              "      <td>4.19</td>\n",
              "      <td>7.776</td>\n",
              "      <td>5.651</td>\n",
              "      <td>4.1</td>\n",
              "      <td>4.08</td>\n",
              "      <td>7.904</td>\n",
              "      <td>7.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1A60</td>\n",
              "      <td>stack</td>\n",
              "      <td>4</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>ADE</td>\n",
              "      <td>3</td>\n",
              "      <td>92.2</td>\n",
              "      <td>75</td>\n",
              "      <td>72</td>\n",
              "      <td>81.2</td>\n",
              "      <td>65.8</td>\n",
              "      <td>153.7</td>\n",
              "      <td>118.8</td>\n",
              "      <td>157.085</td>\n",
              "      <td>139.1</td>\n",
              "      <td>5.96</td>\n",
              "      <td>4.57</td>\n",
              "      <td>4.53</td>\n",
              "      <td>4.54</td>\n",
              "      <td>7.52</td>\n",
              "      <td>5.932</td>\n",
              "      <td>4.28</td>\n",
              "      <td>4.11</td>\n",
              "      <td>8.413</td>\n",
              "      <td>7.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1A60</td>\n",
              "      <td>stack</td>\n",
              "      <td>5</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>4</td>\n",
              "      <td>92.2</td>\n",
              "      <td>74.8</td>\n",
              "      <td>71.8</td>\n",
              "      <td>85.7</td>\n",
              "      <td>65.4</td>\n",
              "      <td>155.07</td>\n",
              "      <td>118.2</td>\n",
              "      <td>161.165</td>\n",
              "      <td>136.1</td>\n",
              "      <td>5.63</td>\n",
              "      <td>4.43</td>\n",
              "      <td>4.4</td>\n",
              "      <td>4.18</td>\n",
              "      <td>7.793</td>\n",
              "      <td>5.988</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.07</td>\n",
              "      <td>8.522</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>5WQ1</td>\n",
              "      <td>stack</td>\n",
              "      <td>19</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>URA</td>\n",
              "      <td>3347</td>\n",
              "      <td>93.656</td>\n",
              "      <td>71.924</td>\n",
              "      <td>72.503</td>\n",
              "      <td>80.33</td>\n",
              "      <td>64.042</td>\n",
              "      <td>152.055</td>\n",
              "      <td>103.523</td>\n",
              "      <td>141.5</td>\n",
              "      <td>139.8</td>\n",
              "      <td>5.494</td>\n",
              "      <td>4.558</td>\n",
              "      <td>4.422</td>\n",
              "      <td>4.561</td>\n",
              "      <td>7.4</td>\n",
              "      <td>5.216</td>\n",
              "      <td>4.469</td>\n",
              "      <td>4.085</td>\n",
              "      <td>7.618</td>\n",
              "      <td>7.914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5WQ1</td>\n",
              "      <td>stack</td>\n",
              "      <td>20</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>3348</td>\n",
              "      <td>92.923</td>\n",
              "      <td>75.273</td>\n",
              "      <td>73.402</td>\n",
              "      <td>82.035</td>\n",
              "      <td>65.331</td>\n",
              "      <td>155.24</td>\n",
              "      <td>118.7</td>\n",
              "      <td>161.6</td>\n",
              "      <td>136.271</td>\n",
              "      <td>5.779</td>\n",
              "      <td>4.496</td>\n",
              "      <td>4.583</td>\n",
              "      <td>4.502</td>\n",
              "      <td>7.947</td>\n",
              "      <td>5.932</td>\n",
              "      <td>4.504</td>\n",
              "      <td>4.127</td>\n",
              "      <td>7.93</td>\n",
              "      <td>7.702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>5WQ1</td>\n",
              "      <td>stack</td>\n",
              "      <td>21</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>URA</td>\n",
              "      <td>3349</td>\n",
              "      <td>93.692</td>\n",
              "      <td>75.31</td>\n",
              "      <td>73.04</td>\n",
              "      <td>82.92</td>\n",
              "      <td>64.482</td>\n",
              "      <td>153.472</td>\n",
              "      <td>102.537</td>\n",
              "      <td>141.916</td>\n",
              "      <td>141.418</td>\n",
              "      <td>5.537</td>\n",
              "      <td>4.481</td>\n",
              "      <td>4.25</td>\n",
              "      <td>4.291</td>\n",
              "      <td>7.043</td>\n",
              "      <td>5.094</td>\n",
              "      <td>4.478</td>\n",
              "      <td>4.08</td>\n",
              "      <td>7.823</td>\n",
              "      <td>8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>5WQ1</td>\n",
              "      <td>stack</td>\n",
              "      <td>22</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>CYT</td>\n",
              "      <td>3350</td>\n",
              "      <td>94.159</td>\n",
              "      <td>75.698</td>\n",
              "      <td>69.764</td>\n",
              "      <td>81.554</td>\n",
              "      <td>64.533</td>\n",
              "      <td>158.72</td>\n",
              "      <td>97.531</td>\n",
              "      <td>141.939</td>\n",
              "      <td>140.551</td>\n",
              "      <td>5.587</td>\n",
              "      <td>4.233</td>\n",
              "      <td>4.471</td>\n",
              "      <td>4.413</td>\n",
              "      <td>7.046</td>\n",
              "      <td>5.634</td>\n",
              "      <td>4.526</td>\n",
              "      <td>4.072</td>\n",
              "      <td>7.894</td>\n",
              "      <td>7.706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>5WQ1</td>\n",
              "      <td>stack</td>\n",
              "      <td>23</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>CYT</td>\n",
              "      <td>3351</td>\n",
              "      <td>92.794</td>\n",
              "      <td>77.513</td>\n",
              "      <td>69.687</td>\n",
              "      <td>83.394</td>\n",
              "      <td>65.073</td>\n",
              "      <td>159.7</td>\n",
              "      <td>97.942</td>\n",
              "      <td>141.78</td>\n",
              "      <td>141.968</td>\n",
              "      <td>5.719</td>\n",
              "      <td>4.064</td>\n",
              "      <td>4.195</td>\n",
              "      <td>4.151</td>\n",
              "      <td>8.1</td>\n",
              "      <td>5.52</td>\n",
              "      <td>4.333</td>\n",
              "      <td>4.012</td>\n",
              "      <td>7.661</td>\n",
              "      <td>7.9548</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3068 rows Ã— 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      id stacking resid orientation  ...     16     17     18      19\n",
              "0   1A60    stack     1        anti  ...   4.07   3.99  8.413    8.03\n",
              "1   1A60    stack     2        anti  ...    4.2   4.15  7.871    7.49\n",
              "2   1A60    stack     3        anti  ...    4.1   4.08  7.904    7.24\n",
              "3   1A60    stack     4        anti  ...   4.28   4.11  8.413    7.68\n",
              "4   1A60    stack     5        anti  ...   4.18   4.07  8.522       7\n",
              "..   ...      ...   ...         ...  ...    ...    ...    ...     ...\n",
              "18  5WQ1    stack    19        anti  ...  4.469  4.085  7.618   7.914\n",
              "19  5WQ1    stack    20        anti  ...  4.504  4.127   7.93   7.702\n",
              "20  5WQ1    stack    21        anti  ...  4.478   4.08  7.823     8.3\n",
              "21  5WQ1    stack    22        anti  ...  4.526  4.072  7.894   7.706\n",
              "22  5WQ1    stack    23        anti  ...  4.333  4.012  7.661  7.9548\n",
              "\n",
              "[3068 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atgdfe35oEFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encode the data in the stacking column\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "cs_all['stacking']= labelencoder.fit_transform(cs_all['stacking'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBtGocD-tQ5K",
        "colab_type": "code",
        "outputId": "78c787ed-7711-461a-9c4e-10eb5984038e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "#cs_all dataset with encoded stacking data\n",
        "cs_all.head(n=10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>stacking</th>\n",
              "      <th>resid</th>\n",
              "      <th>orientation</th>\n",
              "      <th>pseudoknot</th>\n",
              "      <th>base_pairing</th>\n",
              "      <th>sugar_puckering</th>\n",
              "      <th>R</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1A60</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>0</td>\n",
              "      <td>92.7</td>\n",
              "      <td>74.9</td>\n",
              "      <td>72</td>\n",
              "      <td>82.5</td>\n",
              "      <td>65.4</td>\n",
              "      <td>153.6</td>\n",
              "      <td>109.8</td>\n",
              "      <td>157</td>\n",
              "      <td>142.1</td>\n",
              "      <td>5.65</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.12</td>\n",
              "      <td>4.23</td>\n",
              "      <td>7.469</td>\n",
              "      <td>6.124</td>\n",
              "      <td>4.07</td>\n",
              "      <td>3.99</td>\n",
              "      <td>8.413</td>\n",
              "      <td>8.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1A60</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>1</td>\n",
              "      <td>91.9</td>\n",
              "      <td>75.7</td>\n",
              "      <td>72.4</td>\n",
              "      <td>82.3</td>\n",
              "      <td>66.1</td>\n",
              "      <td>155.207</td>\n",
              "      <td>118.377</td>\n",
              "      <td>160.7</td>\n",
              "      <td>141.4</td>\n",
              "      <td>5.88</td>\n",
              "      <td>4.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.31</td>\n",
              "      <td>7.83</td>\n",
              "      <td>5.549</td>\n",
              "      <td>4.2</td>\n",
              "      <td>4.15</td>\n",
              "      <td>7.871</td>\n",
              "      <td>7.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1A60</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>2</td>\n",
              "      <td>92.2</td>\n",
              "      <td>76.1</td>\n",
              "      <td>72.3</td>\n",
              "      <td>84.7</td>\n",
              "      <td>66</td>\n",
              "      <td>154.079</td>\n",
              "      <td>120.289</td>\n",
              "      <td>160.7</td>\n",
              "      <td>141.2</td>\n",
              "      <td>5.77</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.09</td>\n",
              "      <td>4.19</td>\n",
              "      <td>7.776</td>\n",
              "      <td>5.651</td>\n",
              "      <td>4.1</td>\n",
              "      <td>4.08</td>\n",
              "      <td>7.904</td>\n",
              "      <td>7.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1A60</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>ADE</td>\n",
              "      <td>3</td>\n",
              "      <td>92.2</td>\n",
              "      <td>75</td>\n",
              "      <td>72</td>\n",
              "      <td>81.2</td>\n",
              "      <td>65.8</td>\n",
              "      <td>153.7</td>\n",
              "      <td>118.8</td>\n",
              "      <td>157.085</td>\n",
              "      <td>139.1</td>\n",
              "      <td>5.96</td>\n",
              "      <td>4.57</td>\n",
              "      <td>4.53</td>\n",
              "      <td>4.54</td>\n",
              "      <td>7.52</td>\n",
              "      <td>5.932</td>\n",
              "      <td>4.28</td>\n",
              "      <td>4.11</td>\n",
              "      <td>8.413</td>\n",
              "      <td>7.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1A60</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>0</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>GUA</td>\n",
              "      <td>4</td>\n",
              "      <td>92.2</td>\n",
              "      <td>74.8</td>\n",
              "      <td>71.8</td>\n",
              "      <td>85.7</td>\n",
              "      <td>65.4</td>\n",
              "      <td>155.07</td>\n",
              "      <td>118.2</td>\n",
              "      <td>161.165</td>\n",
              "      <td>136.1</td>\n",
              "      <td>5.63</td>\n",
              "      <td>4.43</td>\n",
              "      <td>4.4</td>\n",
              "      <td>4.18</td>\n",
              "      <td>7.793</td>\n",
              "      <td>5.988</td>\n",
              "      <td>4.18</td>\n",
              "      <td>4.07</td>\n",
              "      <td>8.522</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1A60</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>1</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>CYT</td>\n",
              "      <td>5</td>\n",
              "      <td>93.1</td>\n",
              "      <td>76</td>\n",
              "      <td>71.8</td>\n",
              "      <td>82.2</td>\n",
              "      <td>64</td>\n",
              "      <td>159.7</td>\n",
              "      <td>96.5</td>\n",
              "      <td>141.7</td>\n",
              "      <td>141.851</td>\n",
              "      <td>5.64</td>\n",
              "      <td>4.17</td>\n",
              "      <td>4.42</td>\n",
              "      <td>4.32</td>\n",
              "      <td>7.4</td>\n",
              "      <td>5.11</td>\n",
              "      <td>4.32</td>\n",
              "      <td>4.17</td>\n",
              "      <td>7.48</td>\n",
              "      <td>8.169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1A60</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>1</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>URA</td>\n",
              "      <td>6</td>\n",
              "      <td>92.4</td>\n",
              "      <td>74.8</td>\n",
              "      <td>73.5</td>\n",
              "      <td>83.1</td>\n",
              "      <td>65.5</td>\n",
              "      <td>150.76</td>\n",
              "      <td>104.5</td>\n",
              "      <td>143.7</td>\n",
              "      <td>140.758</td>\n",
              "      <td>5.88</td>\n",
              "      <td>4.35</td>\n",
              "      <td>4.65</td>\n",
              "      <td>4.38</td>\n",
              "      <td>6.077</td>\n",
              "      <td>5.82</td>\n",
              "      <td>4.14</td>\n",
              "      <td>4.13</td>\n",
              "      <td>7.89</td>\n",
              "      <td>8.291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1A60</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>1</td>\n",
              "      <td>~C2'-endo</td>\n",
              "      <td>CYT</td>\n",
              "      <td>7</td>\n",
              "      <td>93.4</td>\n",
              "      <td>75.1</td>\n",
              "      <td>72.5</td>\n",
              "      <td>82.3</td>\n",
              "      <td>64.2</td>\n",
              "      <td>155.24</td>\n",
              "      <td>97.7</td>\n",
              "      <td>143.1</td>\n",
              "      <td>138.179</td>\n",
              "      <td>5.5</td>\n",
              "      <td>4.43</td>\n",
              "      <td>4.3</td>\n",
              "      <td>4.29</td>\n",
              "      <td>7.33</td>\n",
              "      <td>5.76</td>\n",
              "      <td>4.08</td>\n",
              "      <td>3.98</td>\n",
              "      <td>7.74</td>\n",
              "      <td>7.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1A60</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>1</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>ADE</td>\n",
              "      <td>8</td>\n",
              "      <td>91.4</td>\n",
              "      <td>75.4</td>\n",
              "      <td>73.3</td>\n",
              "      <td>82.4</td>\n",
              "      <td>66</td>\n",
              "      <td>154.9</td>\n",
              "      <td>118.176</td>\n",
              "      <td>158.057</td>\n",
              "      <td>141.1</td>\n",
              "      <td>6.02</td>\n",
              "      <td>4.67</td>\n",
              "      <td>4.87</td>\n",
              "      <td>4.39</td>\n",
              "      <td>7.86</td>\n",
              "      <td>6.081</td>\n",
              "      <td>4.3</td>\n",
              "      <td>4.26</td>\n",
              "      <td>7.968</td>\n",
              "      <td>8.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1A60</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>anti</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "      <td>1</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>ADE</td>\n",
              "      <td>9</td>\n",
              "      <td>91.5</td>\n",
              "      <td>75</td>\n",
              "      <td>73.6</td>\n",
              "      <td>82.9</td>\n",
              "      <td>66.2</td>\n",
              "      <td>154.9</td>\n",
              "      <td>120.239</td>\n",
              "      <td>157.085</td>\n",
              "      <td>140</td>\n",
              "      <td>5.36</td>\n",
              "      <td>4.35</td>\n",
              "      <td>4.52</td>\n",
              "      <td>4.49</td>\n",
              "      <td>8.04</td>\n",
              "      <td>6.081</td>\n",
              "      <td>4.35</td>\n",
              "      <td>4.33</td>\n",
              "      <td>7.93</td>\n",
              "      <td>7.66</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  stacking resid orientation  ...    16    17     18     19\n",
              "0  1A60         1     1        anti  ...  4.07  3.99  8.413   8.03\n",
              "1  1A60         1     2        anti  ...   4.2  4.15  7.871   7.49\n",
              "2  1A60         1     3        anti  ...   4.1  4.08  7.904   7.24\n",
              "3  1A60         1     4        anti  ...  4.28  4.11  8.413   7.68\n",
              "4  1A60         1     5        anti  ...  4.18  4.07  8.522      7\n",
              "5  1A60         0     6        anti  ...  4.32  4.17   7.48  8.169\n",
              "6  1A60         0     7        anti  ...  4.14  4.13   7.89  8.291\n",
              "7  1A60         0     8        anti  ...  4.08  3.98   7.74    7.6\n",
              "8  1A60         1     9        anti  ...   4.3  4.26  7.968   8.05\n",
              "9  1A60         1    10        anti  ...  4.35  4.33   7.93   7.66\n",
              "\n",
              "[10 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRQkUb7ZTlbW",
        "colab_type": "code",
        "outputId": "4b55477b-31a0-4567-a0c6-cddc87930eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "#original dataset\n",
        "c"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>resid</th>\n",
              "      <th>id</th>\n",
              "      <th>resname</th>\n",
              "      <th>C1p</th>\n",
              "      <th>C2p</th>\n",
              "      <th>C3p</th>\n",
              "      <th>C4p</th>\n",
              "      <th>C5p</th>\n",
              "      <th>C2</th>\n",
              "      <th>C5</th>\n",
              "      <th>C6</th>\n",
              "      <th>C8</th>\n",
              "      <th>H1p</th>\n",
              "      <th>H2p</th>\n",
              "      <th>H3p</th>\n",
              "      <th>H4p</th>\n",
              "      <th>H2</th>\n",
              "      <th>H5</th>\n",
              "      <th>H5p</th>\n",
              "      <th>H5pp</th>\n",
              "      <th>H6</th>\n",
              "      <th>H8</th>\n",
              "      <th>base_pairing</th>\n",
              "      <th>orientation</th>\n",
              "      <th>sugar_puckering</th>\n",
              "      <th>stacking</th>\n",
              "      <th>pseudoknot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>92.700</td>\n",
              "      <td>74.900</td>\n",
              "      <td>72.000</td>\n",
              "      <td>82.500</td>\n",
              "      <td>65.400</td>\n",
              "      <td>153.600</td>\n",
              "      <td>109.800</td>\n",
              "      <td>157.000</td>\n",
              "      <td>142.100</td>\n",
              "      <td>5.650</td>\n",
              "      <td>4.500</td>\n",
              "      <td>4.120</td>\n",
              "      <td>4.230</td>\n",
              "      <td>7.469</td>\n",
              "      <td>6.124</td>\n",
              "      <td>4.070</td>\n",
              "      <td>3.990</td>\n",
              "      <td>8.413</td>\n",
              "      <td>8.0300</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>91.900</td>\n",
              "      <td>75.700</td>\n",
              "      <td>72.400</td>\n",
              "      <td>82.300</td>\n",
              "      <td>66.100</td>\n",
              "      <td>155.207</td>\n",
              "      <td>118.377</td>\n",
              "      <td>160.700</td>\n",
              "      <td>141.400</td>\n",
              "      <td>5.880</td>\n",
              "      <td>4.600</td>\n",
              "      <td>4.500</td>\n",
              "      <td>4.310</td>\n",
              "      <td>7.830</td>\n",
              "      <td>5.549</td>\n",
              "      <td>4.200</td>\n",
              "      <td>4.150</td>\n",
              "      <td>7.871</td>\n",
              "      <td>7.4900</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>92.200</td>\n",
              "      <td>76.100</td>\n",
              "      <td>72.300</td>\n",
              "      <td>84.700</td>\n",
              "      <td>66.000</td>\n",
              "      <td>154.079</td>\n",
              "      <td>120.289</td>\n",
              "      <td>160.700</td>\n",
              "      <td>141.200</td>\n",
              "      <td>5.770</td>\n",
              "      <td>4.500</td>\n",
              "      <td>4.090</td>\n",
              "      <td>4.190</td>\n",
              "      <td>7.776</td>\n",
              "      <td>5.651</td>\n",
              "      <td>4.100</td>\n",
              "      <td>4.080</td>\n",
              "      <td>7.904</td>\n",
              "      <td>7.2400</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>92.200</td>\n",
              "      <td>75.000</td>\n",
              "      <td>72.000</td>\n",
              "      <td>81.200</td>\n",
              "      <td>65.800</td>\n",
              "      <td>153.700</td>\n",
              "      <td>118.800</td>\n",
              "      <td>157.085</td>\n",
              "      <td>139.100</td>\n",
              "      <td>5.960</td>\n",
              "      <td>4.570</td>\n",
              "      <td>4.530</td>\n",
              "      <td>4.540</td>\n",
              "      <td>7.520</td>\n",
              "      <td>5.932</td>\n",
              "      <td>4.280</td>\n",
              "      <td>4.110</td>\n",
              "      <td>8.413</td>\n",
              "      <td>7.6800</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>92.200</td>\n",
              "      <td>74.800</td>\n",
              "      <td>71.800</td>\n",
              "      <td>85.700</td>\n",
              "      <td>65.400</td>\n",
              "      <td>155.070</td>\n",
              "      <td>118.200</td>\n",
              "      <td>161.165</td>\n",
              "      <td>136.100</td>\n",
              "      <td>5.630</td>\n",
              "      <td>4.430</td>\n",
              "      <td>4.400</td>\n",
              "      <td>4.180</td>\n",
              "      <td>7.793</td>\n",
              "      <td>5.988</td>\n",
              "      <td>4.180</td>\n",
              "      <td>4.070</td>\n",
              "      <td>8.522</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3063</th>\n",
              "      <td>3347</td>\n",
              "      <td>19</td>\n",
              "      <td>5WQ1</td>\n",
              "      <td>URA</td>\n",
              "      <td>93.656</td>\n",
              "      <td>71.924</td>\n",
              "      <td>72.503</td>\n",
              "      <td>80.330</td>\n",
              "      <td>64.042</td>\n",
              "      <td>152.055</td>\n",
              "      <td>103.523</td>\n",
              "      <td>141.500</td>\n",
              "      <td>139.800</td>\n",
              "      <td>5.494</td>\n",
              "      <td>4.558</td>\n",
              "      <td>4.422</td>\n",
              "      <td>4.561</td>\n",
              "      <td>7.400</td>\n",
              "      <td>5.216</td>\n",
              "      <td>4.469</td>\n",
              "      <td>4.085</td>\n",
              "      <td>7.618</td>\n",
              "      <td>7.9140</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3064</th>\n",
              "      <td>3348</td>\n",
              "      <td>20</td>\n",
              "      <td>5WQ1</td>\n",
              "      <td>GUA</td>\n",
              "      <td>92.923</td>\n",
              "      <td>75.273</td>\n",
              "      <td>73.402</td>\n",
              "      <td>82.035</td>\n",
              "      <td>65.331</td>\n",
              "      <td>155.240</td>\n",
              "      <td>118.700</td>\n",
              "      <td>161.600</td>\n",
              "      <td>136.271</td>\n",
              "      <td>5.779</td>\n",
              "      <td>4.496</td>\n",
              "      <td>4.583</td>\n",
              "      <td>4.502</td>\n",
              "      <td>7.947</td>\n",
              "      <td>5.932</td>\n",
              "      <td>4.504</td>\n",
              "      <td>4.127</td>\n",
              "      <td>7.930</td>\n",
              "      <td>7.7020</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3065</th>\n",
              "      <td>3349</td>\n",
              "      <td>21</td>\n",
              "      <td>5WQ1</td>\n",
              "      <td>URA</td>\n",
              "      <td>93.692</td>\n",
              "      <td>75.310</td>\n",
              "      <td>73.040</td>\n",
              "      <td>82.920</td>\n",
              "      <td>64.482</td>\n",
              "      <td>153.472</td>\n",
              "      <td>102.537</td>\n",
              "      <td>141.916</td>\n",
              "      <td>141.418</td>\n",
              "      <td>5.537</td>\n",
              "      <td>4.481</td>\n",
              "      <td>4.250</td>\n",
              "      <td>4.291</td>\n",
              "      <td>7.043</td>\n",
              "      <td>5.094</td>\n",
              "      <td>4.478</td>\n",
              "      <td>4.080</td>\n",
              "      <td>7.823</td>\n",
              "      <td>8.3000</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3066</th>\n",
              "      <td>3350</td>\n",
              "      <td>22</td>\n",
              "      <td>5WQ1</td>\n",
              "      <td>CYT</td>\n",
              "      <td>94.159</td>\n",
              "      <td>75.698</td>\n",
              "      <td>69.764</td>\n",
              "      <td>81.554</td>\n",
              "      <td>64.533</td>\n",
              "      <td>158.720</td>\n",
              "      <td>97.531</td>\n",
              "      <td>141.939</td>\n",
              "      <td>140.551</td>\n",
              "      <td>5.587</td>\n",
              "      <td>4.233</td>\n",
              "      <td>4.471</td>\n",
              "      <td>4.413</td>\n",
              "      <td>7.046</td>\n",
              "      <td>5.634</td>\n",
              "      <td>4.526</td>\n",
              "      <td>4.072</td>\n",
              "      <td>7.894</td>\n",
              "      <td>7.7060</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3067</th>\n",
              "      <td>3351</td>\n",
              "      <td>23</td>\n",
              "      <td>5WQ1</td>\n",
              "      <td>CYT</td>\n",
              "      <td>92.794</td>\n",
              "      <td>77.513</td>\n",
              "      <td>69.687</td>\n",
              "      <td>83.394</td>\n",
              "      <td>65.073</td>\n",
              "      <td>159.700</td>\n",
              "      <td>97.942</td>\n",
              "      <td>141.780</td>\n",
              "      <td>141.968</td>\n",
              "      <td>5.719</td>\n",
              "      <td>4.064</td>\n",
              "      <td>4.195</td>\n",
              "      <td>4.151</td>\n",
              "      <td>8.100</td>\n",
              "      <td>5.520</td>\n",
              "      <td>4.333</td>\n",
              "      <td>4.012</td>\n",
              "      <td>7.661</td>\n",
              "      <td>7.9548</td>\n",
              "      <td>0</td>\n",
              "      <td>anti</td>\n",
              "      <td>~C3'-endo</td>\n",
              "      <td>stack</td>\n",
              "      <td>non-pseudoknotted</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3068 rows Ã— 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  resid    id  ... sugar_puckering  stacking         pseudoknot\n",
              "0              0      1  1A60  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "1              1      2  1A60  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "2              2      3  1A60  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "3              3      4  1A60  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "4              4      5  1A60  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "...          ...    ...   ...  ...             ...       ...                ...\n",
              "3063        3347     19  5WQ1  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "3064        3348     20  5WQ1  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "3065        3349     21  5WQ1  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "3066        3350     22  5WQ1  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "3067        3351     23  5WQ1  ...       ~C3'-endo     stack  non-pseudoknotted\n",
              "\n",
              "[3068 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF1FkvnZdApz",
        "colab_type": "text"
      },
      "source": [
        "# MLP model (modified from base-pairing version)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zgfE6mnTD2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_training_testing(cs, leave_out = \"2KOC\", target_name = 'stacking', neighbors = 0, drop_names = ['id', 'base_pairing', 'resid', 'orientation', 'sugar_puckering', 'pseudoknot']):\n",
        "  '''    \n",
        "    This function creates a training and testing set using leave one out    \n",
        "  '''\n",
        "  \n",
        "  # drop extraneous data  \n",
        "  drop_names = drop_names + list(write_out_resname(neighbors))  \n",
        "  \n",
        "  # does not contain leave_out\n",
        "  train = cs[(cs.id != leave_out)]\n",
        "  trainX = train.drop(drop_names, axis=1)\n",
        "  trainy = train[target_name]\n",
        " \n",
        "  # only contains leave_out\n",
        "  test = cs[(cs.id == leave_out)]\n",
        "  testX = test.drop(drop_names, axis=1)\n",
        "  testy = test[target_name]\n",
        "  \n",
        "  # return training and testing data\n",
        "  return(trainX.values, trainy.values, testX.values, testy.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_mLE7iub2pG",
        "colab_type": "code",
        "outputId": "c64f0f3e-2c33-464d-e241-a6f28f6ad80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "NEIGHBORS = 0\n",
        "id = '2KOC'\n",
        "trainX, trainy, testX, testy = create_training_testing(cs_all, leave_out = id, neighbors = NEIGHBORS)\n",
        "print(\"[INFO]: created training and testing data structures\")\n",
        "\n",
        "# setup scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(trainX)\n",
        "\n",
        "# transform input\n",
        "trainX_scaled = scaler.transform(trainX)\n",
        "testX_scaled = scaler.transform(testX)\n",
        "print(\"[INFO]: scaled the features\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: created training and testing data structures\n",
            "[INFO]: scaled the features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFmH9kjymPI",
        "colab_type": "text"
      },
      "source": [
        "**MLP Classifier model w/o optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oj8iXFgGxP48",
        "colab": {}
      },
      "source": [
        "#split the data into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X=cs_all[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]] #Chemical shift data\n",
        "y=cs_all['stacking'] #one-hot encoded stacking data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZLBZ8xk-xP4_",
        "colab": {}
      },
      "source": [
        "#Scale the features\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "trainX_scaled = scaler.transform(X_train)\n",
        "testX_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "003167c4-f3c8-4e08-84b3-92dfbc4d87cf",
        "id": "fPjVJlClxP5B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#fit and evaluate a baseline MLP model w/o optimization\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "classifier = MLPClassifier(max_iter=100)\n",
        "classifier.fit(trainX_scaled, y_train)\n",
        "y_pred = classifier.predict(testX_scaled)\n",
        "print('score report for predicting stacking')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score report for predicting stacking\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.20      0.27       125\n",
            "           1       0.88      0.96      0.92       796\n",
            "\n",
            "    accuracy                           0.85       921\n",
            "   macro avg       0.65      0.58      0.60       921\n",
            "weighted avg       0.82      0.85      0.83       921\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr97UNg0yEp-",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameter Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARbb60WBe15p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(50,50,50,50), (50,50,50), (50,50), (50,)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "}\n",
        "\n",
        "from scipy.stats import randint as sp_randint\n",
        "from scipy.stats import expon as sp_expon\n",
        "\n",
        "min_size, max_size = 5, 100\n",
        "parameter_space_distribution = {\n",
        "    'hidden_layer_sizes': [(sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),sp_randint.rvs(min_size, max_size)), (sp_randint.rvs(min_size, max_size),)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
        "    'alpha': sp_expon(scale=.01),\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "    'learning_rate_init': sp_expon(scale=.001),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXblhawQmgVy",
        "colab_type": "code",
        "outputId": "25ca7667-e8e2-4c42-a8b2-f07df939cbc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(classifier, param_distributions=parameter_space_distribution, n_iter=n_iter_search, cv=4, verbose = 5)\n",
        "random_search.fit(trainX_scaled, y_train)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
            "[CV] activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs, score=0.816, total=   0.9s\n",
            "[CV] activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs, score=0.799, total=   1.0s\n",
            "[CV] activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs, score=0.808, total=   0.8s\n",
            "[CV] activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    2.7s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.009507650979680862, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0013316667694332983, solver=lbfgs, score=0.841, total=   0.8s\n",
            "[CV] activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    3.5s remaining:    0.0s\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd, score=0.868, total=   0.9s\n",
            "[CV] activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd, score=0.868, total=   0.9s\n",
            "[CV] activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd, score=0.868, total=   0.9s\n",
            "[CV] activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.015321706819395813, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0005550267954362299, solver=sgd, score=0.869, total=   0.9s\n",
            "[CV] activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs, score=0.812, total=   1.0s\n",
            "[CV] activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs, score=0.806, total=   0.9s\n",
            "[CV] activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs, score=0.812, total=   1.0s\n",
            "[CV] activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.01855240847284999, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=0.0008856702905764821, solver=lbfgs, score=0.836, total=   1.0s\n",
            "[CV] activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd, score=0.868, total=   1.6s\n",
            "[CV] activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd, score=0.866, total=   1.6s\n",
            "[CV] activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd, score=0.866, total=   1.6s\n",
            "[CV] activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0006701475720185195, hidden_layer_sizes=(25, 77), learning_rate=constant, learning_rate_init=0.00029773297144909997, solver=sgd, score=0.869, total=   1.6s\n",
            "[CV] activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs, score=0.814, total=   1.2s\n",
            "[CV] activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs, score=0.803, total=   1.5s\n",
            "[CV] activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs, score=0.819, total=   1.5s\n",
            "[CV] activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.006938617890269532, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0004556391675146321, solver=lbfgs, score=0.812, total=   1.6s\n",
            "[CV] activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd, score=0.872, total=   2.1s\n",
            "[CV] activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd, score=0.873, total=   2.1s\n",
            "[CV] activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd, score=0.873, total=   2.2s\n",
            "[CV] activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.00995238240786426, hidden_layer_sizes=(32, 77, 19), learning_rate=adaptive, learning_rate_init=0.0015258010656708403, solver=sgd, score=0.875, total=   2.2s\n",
            "[CV] activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam, score=0.868, total=   1.0s\n",
            "[CV] activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam, score=0.862, total=   0.9s\n",
            "[CV] activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam, score=0.868, total=   1.0s\n",
            "[CV] activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.012709947660994054, hidden_layer_sizes=(75,), learning_rate=constant, learning_rate_init=9.717748924947379e-05, solver=adam, score=0.869, total=   1.0s\n",
            "[CV] activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd, score=0.868, total=   1.2s\n",
            "[CV] activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd, score=0.860, total=   1.2s\n",
            "[CV] activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd, score=0.862, total=   1.2s\n",
            "[CV] activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=tanh, alpha=0.0015623348460095166, hidden_layer_sizes=(75,), learning_rate=adaptive, learning_rate_init=0.0003862117443568601, solver=sgd, score=0.869, total=   1.2s\n",
            "[CV] activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs, score=0.804, total=   1.7s\n",
            "[CV] activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs, score=0.797, total=   1.8s\n",
            "[CV] activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs, score=0.801, total=   1.6s\n",
            "[CV] activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs \n",
            "[CV]  activation=tanh, alpha=0.018418215919551412, hidden_layer_sizes=(32, 77, 19), learning_rate=constant, learning_rate_init=0.00019137609768365554, solver=lbfgs, score=0.815, total=   1.8s\n",
            "[CV] activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam, score=0.866, total=   1.3s\n",
            "[CV] activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam, score=0.873, total=   1.3s\n",
            "[CV] activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam, score=0.872, total=   1.4s\n",
            "[CV] activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   52.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  activation=relu, alpha=0.00826708271939206, hidden_layer_sizes=(25, 77), learning_rate=adaptive, learning_rate_init=0.001043416961188233, solver=adam, score=0.871, total=   1.4s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=4, error_score='raise-deprecating',\n",
              "                   estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                           batch_size='auto', beta_1=0.9,\n",
              "                                           beta_2=0.999, early_stopping=False,\n",
              "                                           epsilon=1e-08,\n",
              "                                           hidden_layer_sizes=(100,),\n",
              "                                           learning_rate='constant',\n",
              "                                           learning_rate_init=0.001,\n",
              "                                           max_iter=100, momentum=0.9,\n",
              "                                           n_iter_no_change=10,\n",
              "                                           nesterovs_momentum=True, power_t=0.5,\n",
              "                                           rand...\n",
              "                                        'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4e964d94a8>,\n",
              "                                        'hidden_layer_sizes': [(97, 73, 34, 7),\n",
              "                                                               (32, 77, 19),\n",
              "                                                               (25, 77),\n",
              "                                                               (75,)],\n",
              "                                        'learning_rate': ['constant',\n",
              "                                                          'adaptive'],\n",
              "                                        'learning_rate_init': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f4e96af40b8>,\n",
              "                                        'solver': ['sgd', 'adam', 'lbfgs']},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InimBki8lsjw",
        "colab_type": "code",
        "outputId": "809ba149-2043-4eaf-ce7d-9c7acd39f586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Best parameter set\n",
        "print('Best parameters found:\\n', random_search.best_params_)\n",
        "\n",
        "# All results\n",
        "means = random_search.cv_results_['mean_test_score']\n",
        "stds = random_search.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, random_search.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:\n",
            " {'activation': 'tanh', 'alpha': 0.00995238240786426, 'hidden_layer_sizes': (32, 77, 19), 'learning_rate': 'adaptive', 'learning_rate_init': 0.0015258010656708403, 'solver': 'sgd'}\n",
            "0.816 (+/-0.032) for {'activation': 'tanh', 'alpha': 0.009507650979680862, 'hidden_layer_sizes': (75,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.0013316667694332983, 'solver': 'lbfgs'}\n",
            "0.868 (+/-0.001) for {'activation': 'relu', 'alpha': 0.015321706819395813, 'hidden_layer_sizes': (75,), 'learning_rate': 'constant', 'learning_rate_init': 0.0005550267954362299, 'solver': 'sgd'}\n",
            "0.816 (+/-0.023) for {'activation': 'tanh', 'alpha': 0.01855240847284999, 'hidden_layer_sizes': (75,), 'learning_rate': 'constant', 'learning_rate_init': 0.0008856702905764821, 'solver': 'lbfgs'}\n",
            "0.867 (+/-0.003) for {'activation': 'tanh', 'alpha': 0.0006701475720185195, 'hidden_layer_sizes': (25, 77), 'learning_rate': 'constant', 'learning_rate_init': 0.00029773297144909997, 'solver': 'sgd'}\n",
            "0.812 (+/-0.012) for {'activation': 'tanh', 'alpha': 0.006938617890269532, 'hidden_layer_sizes': (32, 77, 19), 'learning_rate': 'adaptive', 'learning_rate_init': 0.0004556391675146321, 'solver': 'lbfgs'}\n",
            "0.873 (+/-0.002) for {'activation': 'tanh', 'alpha': 0.00995238240786426, 'hidden_layer_sizes': (32, 77, 19), 'learning_rate': 'adaptive', 'learning_rate_init': 0.0015258010656708403, 'solver': 'sgd'}\n",
            "0.867 (+/-0.005) for {'activation': 'relu', 'alpha': 0.012709947660994054, 'hidden_layer_sizes': (75,), 'learning_rate': 'constant', 'learning_rate_init': 9.717748924947379e-05, 'solver': 'adam'}\n",
            "0.865 (+/-0.008) for {'activation': 'tanh', 'alpha': 0.0015623348460095166, 'hidden_layer_sizes': (75,), 'learning_rate': 'adaptive', 'learning_rate_init': 0.0003862117443568601, 'solver': 'sgd'}\n",
            "0.804 (+/-0.014) for {'activation': 'tanh', 'alpha': 0.018418215919551412, 'hidden_layer_sizes': (32, 77, 19), 'learning_rate': 'constant', 'learning_rate_init': 0.00019137609768365554, 'solver': 'lbfgs'}\n",
            "0.871 (+/-0.006) for {'activation': 'relu', 'alpha': 0.00826708271939206, 'hidden_layer_sizes': (25, 77), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001043416961188233, 'solver': 'adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFjkSLrflwTe",
        "colab_type": "code",
        "outputId": "6ed953a0-a8bf-4972-86c0-5581c67b3304",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#score report for model optimized via random search\n",
        "y_true, y_pred = y_test , random_search.predict(testX_scaled)\n",
        "print('Results on the test set:')\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results on the test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.11      0.18       125\n",
            "           1       0.88      0.98      0.92       796\n",
            "\n",
            "    accuracy                           0.86       921\n",
            "   macro avg       0.67      0.55      0.55       921\n",
            "weighted avg       0.82      0.86      0.82       921\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-yLnzCsczib",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_3UbZPlRKgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split data into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X=cs_all[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]] #Chemical shift data\n",
        "y=cs_all['stacking'] #one-hot encoded stacking data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmU1EzwSqkHt",
        "colab_type": "code",
        "outputId": "c73d4881-acb6-404b-defc-6f9dda3d8a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "#fit and evaluate baseline unoptimized random forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "sklearn_model = RandomForestClassifier(n_estimators=100)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred=sklearn_model.predict(X_test)\n",
        "# Evaluate it.\n",
        "print('f1 score for predicting stacking')\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score for predicting stacking\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.13      0.22       123\n",
            "           1       0.88      0.99      0.93       798\n",
            "\n",
            "    accuracy                           0.88       921\n",
            "   macro avg       0.80      0.56      0.58       921\n",
            "weighted avg       0.86      0.88      0.84       921\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU4W5WhSpSbU",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameter Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eySc8WpnM9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rmm1C60nOAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrV2RVKNnUeh",
        "colab_type": "code",
        "outputId": "acadc6e5-1794-47df-8a09-f9dc66009afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator = sklearn_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  2.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features='auto',\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    n_estimators=100,\n",
              "                                                    n_jobs=None,\n",
              "                                                    oob_score...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntH5aZx6nrYd",
        "colab_type": "code",
        "outputId": "1c7ab2e1-931c-4ec0-a596-7dccfa7aafbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "y_true, y_pred = y_test , rf_random.predict(X_test)\n",
        "print('Results on the test set:')\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results on the test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.13      0.22       123\n",
            "           1       0.88      0.99      0.93       798\n",
            "\n",
            "    accuracy                           0.87       921\n",
            "   macro avg       0.76      0.56      0.57       921\n",
            "weighted avg       0.85      0.87      0.84       921\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd7s8Ez_o7Am",
        "colab_type": "text"
      },
      "source": [
        "# Stochastic Gradient Descent (SGD) Classifier Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIYEjtr0e4BV",
        "colab_type": "code",
        "outputId": "62563547-fac2-46cc-c349-b0c910b06dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#fit and evaluate unoptimized SGD model\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=100, shuffle=True)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred=clf.predict(X_test)\n",
        "print('f1 score for predicting stacking')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1 score for predicting stacking\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.93      0.25       123\n",
            "           1       0.93      0.15      0.26       798\n",
            "\n",
            "    accuracy                           0.25       921\n",
            "   macro avg       0.54      0.54      0.25       921\n",
            "weighted avg       0.82      0.25      0.26       921\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz2aS8FjpZV9",
        "colab_type": "text"
      },
      "source": [
        "**Hyperparameter Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb0c7PfnlBsg",
        "colab_type": "code",
        "outputId": "4b3425bc-7729-46b0-b714-be368d546b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']\n",
        "penalty = ['l1', 'l2', 'elasticnet']\n",
        "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
        "class_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\n",
        "eta0 = [1, 10, 100]\n",
        "\n",
        "param_distributions = dict(loss=loss,\n",
        "                           penalty=penalty,\n",
        "                           alpha=alpha,\n",
        "                           learning_rate=learning_rate,\n",
        "                           class_weight=class_weight,\n",
        "                           eta0=eta0)\n",
        "random = RandomizedSearchCV(estimator=clf,\n",
        "                            param_distributions=param_distributions,\n",
        "                            scoring=None,\n",
        "                            verbose=1, n_jobs=-1,\n",
        "                            n_iter=10)\n",
        "random_result = random.fit(X_train, y_train)\n",
        "\n",
        "print('Best Score: ', random_result.best_score_)\n",
        "print('Best Params: ', random_result.best_params_)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Score:  0.8672566371681416\n",
            "Best Params:  {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'constant', 'eta0': 100, 'class_weight': {1: 0.7, 0: 0.3}, 'alpha': 10}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    4.2s finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqdwQhWBmQsj",
        "colab_type": "code",
        "outputId": "9df3b802-3f61-44b6-a11e-d7d46636d66c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#evaluation of the optimized model\n",
        "y_true, y_pred = (y_test) , random.predict(X_test)\n",
        "print('Results on the test set:')\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results on the test set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       123\n",
            "           1       0.87      1.00      0.93       798\n",
            "\n",
            "    accuracy                           0.87       921\n",
            "   macro avg       0.43      0.50      0.46       921\n",
            "weighted avg       0.75      0.87      0.80       921\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMQun47-rWSB",
        "colab_type": "text"
      },
      "source": [
        "# SASA Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRN3PeCc3-Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the SASA dataset into a Pandas dataframe\n",
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "url=\"https://drive.google.com/uc?id=1Y3Imx-lTjGKCQAFqEKTbaMSzFARtwEFN\"\n",
        "s=requests.get(url).content\n",
        "sasa=pd.read_csv(io.StringIO(s.decode(\"utf-8\")), sep=\" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCHmFc7v41Py",
        "colab_type": "code",
        "outputId": "36dcadce-7761-4835-e911-ea37be7975e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#drop the values from the sasa data that are not present in the chemical shift dataset\n",
        "sasa=sasa.drop([235,1254,1280,1947,2436,2476],axis=0)\n",
        "pd.set_option('display.max_rows', 4000)\n",
        "display(sasa.head(n=100))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>resname</th>\n",
              "      <th>resid</th>\n",
              "      <th>sasa-All-atoms</th>\n",
              "      <th>sasa-Total-Side</th>\n",
              "      <th>sasa-Main-Chain</th>\n",
              "      <th>sasa-Non-polar</th>\n",
              "      <th>sasa-All-polar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>1</td>\n",
              "      <td>134.15</td>\n",
              "      <td>73.92</td>\n",
              "      <td>60.23</td>\n",
              "      <td>31.95</td>\n",
              "      <td>102.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>2</td>\n",
              "      <td>172.00</td>\n",
              "      <td>56.91</td>\n",
              "      <td>115.09</td>\n",
              "      <td>38.57</td>\n",
              "      <td>133.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>3</td>\n",
              "      <td>187.59</td>\n",
              "      <td>58.90</td>\n",
              "      <td>128.68</td>\n",
              "      <td>52.70</td>\n",
              "      <td>134.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>4</td>\n",
              "      <td>186.44</td>\n",
              "      <td>56.18</td>\n",
              "      <td>130.26</td>\n",
              "      <td>57.02</td>\n",
              "      <td>129.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>5</td>\n",
              "      <td>204.31</td>\n",
              "      <td>87.69</td>\n",
              "      <td>116.62</td>\n",
              "      <td>37.60</td>\n",
              "      <td>166.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>6</td>\n",
              "      <td>180.22</td>\n",
              "      <td>63.00</td>\n",
              "      <td>117.22</td>\n",
              "      <td>52.77</td>\n",
              "      <td>127.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>7</td>\n",
              "      <td>181.17</td>\n",
              "      <td>73.80</td>\n",
              "      <td>107.38</td>\n",
              "      <td>65.07</td>\n",
              "      <td>116.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>8</td>\n",
              "      <td>293.02</td>\n",
              "      <td>145.44</td>\n",
              "      <td>147.59</td>\n",
              "      <td>91.49</td>\n",
              "      <td>201.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>9</td>\n",
              "      <td>233.37</td>\n",
              "      <td>127.57</td>\n",
              "      <td>105.80</td>\n",
              "      <td>85.06</td>\n",
              "      <td>148.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>10</td>\n",
              "      <td>174.18</td>\n",
              "      <td>71.36</td>\n",
              "      <td>102.82</td>\n",
              "      <td>58.00</td>\n",
              "      <td>116.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>11</td>\n",
              "      <td>157.63</td>\n",
              "      <td>30.09</td>\n",
              "      <td>127.53</td>\n",
              "      <td>61.77</td>\n",
              "      <td>95.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>12</td>\n",
              "      <td>196.95</td>\n",
              "      <td>68.89</td>\n",
              "      <td>128.05</td>\n",
              "      <td>49.96</td>\n",
              "      <td>146.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>13</td>\n",
              "      <td>175.30</td>\n",
              "      <td>55.74</td>\n",
              "      <td>119.56</td>\n",
              "      <td>51.25</td>\n",
              "      <td>124.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>14</td>\n",
              "      <td>163.19</td>\n",
              "      <td>43.38</td>\n",
              "      <td>119.81</td>\n",
              "      <td>50.95</td>\n",
              "      <td>112.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>15</td>\n",
              "      <td>164.40</td>\n",
              "      <td>48.12</td>\n",
              "      <td>116.28</td>\n",
              "      <td>48.56</td>\n",
              "      <td>115.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>16</td>\n",
              "      <td>178.88</td>\n",
              "      <td>58.33</td>\n",
              "      <td>120.54</td>\n",
              "      <td>56.56</td>\n",
              "      <td>122.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>17</td>\n",
              "      <td>165.18</td>\n",
              "      <td>41.22</td>\n",
              "      <td>123.96</td>\n",
              "      <td>52.76</td>\n",
              "      <td>112.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>18</td>\n",
              "      <td>165.38</td>\n",
              "      <td>52.22</td>\n",
              "      <td>113.16</td>\n",
              "      <td>50.22</td>\n",
              "      <td>115.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>19</td>\n",
              "      <td>159.37</td>\n",
              "      <td>44.63</td>\n",
              "      <td>114.74</td>\n",
              "      <td>45.91</td>\n",
              "      <td>113.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>20</td>\n",
              "      <td>141.00</td>\n",
              "      <td>26.32</td>\n",
              "      <td>114.67</td>\n",
              "      <td>44.53</td>\n",
              "      <td>96.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>21</td>\n",
              "      <td>167.34</td>\n",
              "      <td>79.45</td>\n",
              "      <td>87.89</td>\n",
              "      <td>32.04</td>\n",
              "      <td>135.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>22</td>\n",
              "      <td>225.84</td>\n",
              "      <td>149.24</td>\n",
              "      <td>76.60</td>\n",
              "      <td>72.85</td>\n",
              "      <td>152.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>23</td>\n",
              "      <td>233.71</td>\n",
              "      <td>154.29</td>\n",
              "      <td>79.41</td>\n",
              "      <td>76.54</td>\n",
              "      <td>157.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>24</td>\n",
              "      <td>120.95</td>\n",
              "      <td>48.76</td>\n",
              "      <td>72.20</td>\n",
              "      <td>57.94</td>\n",
              "      <td>63.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>25</td>\n",
              "      <td>154.16</td>\n",
              "      <td>33.53</td>\n",
              "      <td>120.63</td>\n",
              "      <td>48.52</td>\n",
              "      <td>105.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>26</td>\n",
              "      <td>154.09</td>\n",
              "      <td>38.62</td>\n",
              "      <td>115.47</td>\n",
              "      <td>64.97</td>\n",
              "      <td>89.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>27</td>\n",
              "      <td>158.37</td>\n",
              "      <td>37.67</td>\n",
              "      <td>120.70</td>\n",
              "      <td>50.48</td>\n",
              "      <td>107.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>28</td>\n",
              "      <td>148.66</td>\n",
              "      <td>28.76</td>\n",
              "      <td>119.91</td>\n",
              "      <td>48.21</td>\n",
              "      <td>100.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>29</td>\n",
              "      <td>163.96</td>\n",
              "      <td>47.58</td>\n",
              "      <td>116.38</td>\n",
              "      <td>46.27</td>\n",
              "      <td>117.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>30</td>\n",
              "      <td>163.82</td>\n",
              "      <td>49.71</td>\n",
              "      <td>114.11</td>\n",
              "      <td>34.84</td>\n",
              "      <td>128.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>31</td>\n",
              "      <td>163.55</td>\n",
              "      <td>35.88</td>\n",
              "      <td>127.67</td>\n",
              "      <td>52.86</td>\n",
              "      <td>110.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>32</td>\n",
              "      <td>180.39</td>\n",
              "      <td>57.91</td>\n",
              "      <td>122.48</td>\n",
              "      <td>53.92</td>\n",
              "      <td>126.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>33</td>\n",
              "      <td>261.80</td>\n",
              "      <td>171.07</td>\n",
              "      <td>90.74</td>\n",
              "      <td>71.35</td>\n",
              "      <td>190.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>34</td>\n",
              "      <td>270.72</td>\n",
              "      <td>166.96</td>\n",
              "      <td>103.75</td>\n",
              "      <td>59.95</td>\n",
              "      <td>210.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>35</td>\n",
              "      <td>137.40</td>\n",
              "      <td>31.73</td>\n",
              "      <td>105.68</td>\n",
              "      <td>49.80</td>\n",
              "      <td>87.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1A60</td>\n",
              "      <td>URA</td>\n",
              "      <td>36</td>\n",
              "      <td>140.43</td>\n",
              "      <td>29.27</td>\n",
              "      <td>111.17</td>\n",
              "      <td>50.06</td>\n",
              "      <td>90.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>37</td>\n",
              "      <td>100.76</td>\n",
              "      <td>19.93</td>\n",
              "      <td>80.83</td>\n",
              "      <td>35.63</td>\n",
              "      <td>65.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>38</td>\n",
              "      <td>146.31</td>\n",
              "      <td>51.18</td>\n",
              "      <td>95.13</td>\n",
              "      <td>42.09</td>\n",
              "      <td>104.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1A60</td>\n",
              "      <td>GUA</td>\n",
              "      <td>39</td>\n",
              "      <td>150.98</td>\n",
              "      <td>39.49</td>\n",
              "      <td>111.49</td>\n",
              "      <td>40.67</td>\n",
              "      <td>110.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>40</td>\n",
              "      <td>188.15</td>\n",
              "      <td>39.60</td>\n",
              "      <td>148.55</td>\n",
              "      <td>62.45</td>\n",
              "      <td>125.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>41</td>\n",
              "      <td>169.29</td>\n",
              "      <td>54.71</td>\n",
              "      <td>114.59</td>\n",
              "      <td>41.17</td>\n",
              "      <td>128.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>42</td>\n",
              "      <td>203.02</td>\n",
              "      <td>82.06</td>\n",
              "      <td>120.96</td>\n",
              "      <td>48.05</td>\n",
              "      <td>154.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1A60</td>\n",
              "      <td>CYT</td>\n",
              "      <td>43</td>\n",
              "      <td>244.21</td>\n",
              "      <td>140.82</td>\n",
              "      <td>103.39</td>\n",
              "      <td>51.57</td>\n",
              "      <td>192.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1A60</td>\n",
              "      <td>ADE</td>\n",
              "      <td>44</td>\n",
              "      <td>390.82</td>\n",
              "      <td>199.46</td>\n",
              "      <td>191.36</td>\n",
              "      <td>144.32</td>\n",
              "      <td>246.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>1</td>\n",
              "      <td>238.79</td>\n",
              "      <td>123.44</td>\n",
              "      <td>115.34</td>\n",
              "      <td>92.39</td>\n",
              "      <td>146.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>2</td>\n",
              "      <td>183.70</td>\n",
              "      <td>60.72</td>\n",
              "      <td>122.98</td>\n",
              "      <td>41.92</td>\n",
              "      <td>141.78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>URA</td>\n",
              "      <td>3</td>\n",
              "      <td>165.67</td>\n",
              "      <td>46.87</td>\n",
              "      <td>118.80</td>\n",
              "      <td>51.46</td>\n",
              "      <td>114.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>4</td>\n",
              "      <td>173.47</td>\n",
              "      <td>50.32</td>\n",
              "      <td>123.15</td>\n",
              "      <td>42.97</td>\n",
              "      <td>130.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>5</td>\n",
              "      <td>171.96</td>\n",
              "      <td>47.39</td>\n",
              "      <td>124.57</td>\n",
              "      <td>36.87</td>\n",
              "      <td>135.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>6</td>\n",
              "      <td>153.97</td>\n",
              "      <td>54.68</td>\n",
              "      <td>99.29</td>\n",
              "      <td>22.20</td>\n",
              "      <td>131.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>ADE</td>\n",
              "      <td>7</td>\n",
              "      <td>157.87</td>\n",
              "      <td>35.71</td>\n",
              "      <td>122.16</td>\n",
              "      <td>55.49</td>\n",
              "      <td>102.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>ADE</td>\n",
              "      <td>8</td>\n",
              "      <td>168.95</td>\n",
              "      <td>51.17</td>\n",
              "      <td>117.78</td>\n",
              "      <td>36.48</td>\n",
              "      <td>132.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>9</td>\n",
              "      <td>172.09</td>\n",
              "      <td>61.18</td>\n",
              "      <td>110.91</td>\n",
              "      <td>33.61</td>\n",
              "      <td>138.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>10</td>\n",
              "      <td>175.97</td>\n",
              "      <td>59.27</td>\n",
              "      <td>116.71</td>\n",
              "      <td>42.83</td>\n",
              "      <td>133.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>11</td>\n",
              "      <td>172.66</td>\n",
              "      <td>43.78</td>\n",
              "      <td>128.88</td>\n",
              "      <td>46.34</td>\n",
              "      <td>126.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>12</td>\n",
              "      <td>159.79</td>\n",
              "      <td>39.32</td>\n",
              "      <td>120.47</td>\n",
              "      <td>39.51</td>\n",
              "      <td>120.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>13</td>\n",
              "      <td>196.04</td>\n",
              "      <td>91.30</td>\n",
              "      <td>104.73</td>\n",
              "      <td>29.38</td>\n",
              "      <td>166.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>URA</td>\n",
              "      <td>14</td>\n",
              "      <td>188.42</td>\n",
              "      <td>67.01</td>\n",
              "      <td>121.41</td>\n",
              "      <td>51.97</td>\n",
              "      <td>136.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>15</td>\n",
              "      <td>117.83</td>\n",
              "      <td>14.01</td>\n",
              "      <td>103.81</td>\n",
              "      <td>30.82</td>\n",
              "      <td>87.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>16</td>\n",
              "      <td>301.90</td>\n",
              "      <td>168.48</td>\n",
              "      <td>133.42</td>\n",
              "      <td>76.54</td>\n",
              "      <td>225.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>URA</td>\n",
              "      <td>17</td>\n",
              "      <td>81.08</td>\n",
              "      <td>14.68</td>\n",
              "      <td>66.40</td>\n",
              "      <td>11.54</td>\n",
              "      <td>69.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>18</td>\n",
              "      <td>214.37</td>\n",
              "      <td>99.83</td>\n",
              "      <td>114.54</td>\n",
              "      <td>60.86</td>\n",
              "      <td>153.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>19</td>\n",
              "      <td>140.02</td>\n",
              "      <td>36.51</td>\n",
              "      <td>103.51</td>\n",
              "      <td>39.14</td>\n",
              "      <td>100.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>20</td>\n",
              "      <td>170.86</td>\n",
              "      <td>45.94</td>\n",
              "      <td>124.92</td>\n",
              "      <td>53.20</td>\n",
              "      <td>117.66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>21</td>\n",
              "      <td>167.32</td>\n",
              "      <td>49.12</td>\n",
              "      <td>118.21</td>\n",
              "      <td>50.62</td>\n",
              "      <td>116.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>22</td>\n",
              "      <td>160.81</td>\n",
              "      <td>37.85</td>\n",
              "      <td>122.96</td>\n",
              "      <td>44.69</td>\n",
              "      <td>116.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>23</td>\n",
              "      <td>194.70</td>\n",
              "      <td>74.89</td>\n",
              "      <td>119.81</td>\n",
              "      <td>46.72</td>\n",
              "      <td>147.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>24</td>\n",
              "      <td>191.25</td>\n",
              "      <td>81.05</td>\n",
              "      <td>110.20</td>\n",
              "      <td>40.77</td>\n",
              "      <td>150.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>ADE</td>\n",
              "      <td>25</td>\n",
              "      <td>166.63</td>\n",
              "      <td>55.68</td>\n",
              "      <td>110.95</td>\n",
              "      <td>65.30</td>\n",
              "      <td>101.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>26</td>\n",
              "      <td>175.55</td>\n",
              "      <td>52.94</td>\n",
              "      <td>122.61</td>\n",
              "      <td>40.28</td>\n",
              "      <td>135.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>27</td>\n",
              "      <td>168.88</td>\n",
              "      <td>45.71</td>\n",
              "      <td>123.18</td>\n",
              "      <td>52.50</td>\n",
              "      <td>116.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>GUA</td>\n",
              "      <td>28</td>\n",
              "      <td>183.05</td>\n",
              "      <td>71.54</td>\n",
              "      <td>111.51</td>\n",
              "      <td>43.35</td>\n",
              "      <td>139.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>29</td>\n",
              "      <td>172.66</td>\n",
              "      <td>51.16</td>\n",
              "      <td>121.51</td>\n",
              "      <td>51.99</td>\n",
              "      <td>120.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>1HWQ</td>\n",
              "      <td>CYT</td>\n",
              "      <td>30</td>\n",
              "      <td>282.70</td>\n",
              "      <td>103.02</td>\n",
              "      <td>179.68</td>\n",
              "      <td>94.54</td>\n",
              "      <td>188.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>ADE</td>\n",
              "      <td>1</td>\n",
              "      <td>252.22</td>\n",
              "      <td>155.45</td>\n",
              "      <td>96.77</td>\n",
              "      <td>91.46</td>\n",
              "      <td>160.76</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>GUA</td>\n",
              "      <td>2</td>\n",
              "      <td>150.94</td>\n",
              "      <td>72.24</td>\n",
              "      <td>78.70</td>\n",
              "      <td>47.85</td>\n",
              "      <td>103.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>URA</td>\n",
              "      <td>3</td>\n",
              "      <td>169.40</td>\n",
              "      <td>56.78</td>\n",
              "      <td>112.62</td>\n",
              "      <td>52.89</td>\n",
              "      <td>116.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>ADE</td>\n",
              "      <td>4</td>\n",
              "      <td>184.23</td>\n",
              "      <td>63.22</td>\n",
              "      <td>121.01</td>\n",
              "      <td>66.98</td>\n",
              "      <td>117.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>GUA</td>\n",
              "      <td>5</td>\n",
              "      <td>191.15</td>\n",
              "      <td>64.81</td>\n",
              "      <td>126.33</td>\n",
              "      <td>52.54</td>\n",
              "      <td>138.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>ADE</td>\n",
              "      <td>6</td>\n",
              "      <td>163.90</td>\n",
              "      <td>34.38</td>\n",
              "      <td>129.52</td>\n",
              "      <td>51.20</td>\n",
              "      <td>112.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>ADE</td>\n",
              "      <td>7</td>\n",
              "      <td>191.62</td>\n",
              "      <td>52.32</td>\n",
              "      <td>139.30</td>\n",
              "      <td>58.21</td>\n",
              "      <td>133.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>ADE</td>\n",
              "      <td>8</td>\n",
              "      <td>187.29</td>\n",
              "      <td>50.61</td>\n",
              "      <td>136.67</td>\n",
              "      <td>60.41</td>\n",
              "      <td>126.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>CYT</td>\n",
              "      <td>9</td>\n",
              "      <td>179.32</td>\n",
              "      <td>59.66</td>\n",
              "      <td>119.65</td>\n",
              "      <td>54.85</td>\n",
              "      <td>124.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>ADE</td>\n",
              "      <td>10</td>\n",
              "      <td>203.82</td>\n",
              "      <td>74.24</td>\n",
              "      <td>129.58</td>\n",
              "      <td>58.74</td>\n",
              "      <td>145.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>ADE</td>\n",
              "      <td>11</td>\n",
              "      <td>212.07</td>\n",
              "      <td>80.47</td>\n",
              "      <td>131.60</td>\n",
              "      <td>72.61</td>\n",
              "      <td>139.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>GUA</td>\n",
              "      <td>12</td>\n",
              "      <td>200.20</td>\n",
              "      <td>72.16</td>\n",
              "      <td>128.04</td>\n",
              "      <td>46.40</td>\n",
              "      <td>153.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>GUA</td>\n",
              "      <td>13</td>\n",
              "      <td>191.93</td>\n",
              "      <td>78.93</td>\n",
              "      <td>113.00</td>\n",
              "      <td>42.22</td>\n",
              "      <td>149.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>CYT</td>\n",
              "      <td>14</td>\n",
              "      <td>168.56</td>\n",
              "      <td>53.28</td>\n",
              "      <td>115.28</td>\n",
              "      <td>52.25</td>\n",
              "      <td>116.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>URA</td>\n",
              "      <td>15</td>\n",
              "      <td>104.59</td>\n",
              "      <td>10.78</td>\n",
              "      <td>93.80</td>\n",
              "      <td>29.67</td>\n",
              "      <td>74.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>URA</td>\n",
              "      <td>16</td>\n",
              "      <td>235.10</td>\n",
              "      <td>113.92</td>\n",
              "      <td>121.18</td>\n",
              "      <td>88.94</td>\n",
              "      <td>146.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>CYT</td>\n",
              "      <td>17</td>\n",
              "      <td>202.52</td>\n",
              "      <td>101.14</td>\n",
              "      <td>101.38</td>\n",
              "      <td>33.93</td>\n",
              "      <td>168.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>GUA</td>\n",
              "      <td>18</td>\n",
              "      <td>242.32</td>\n",
              "      <td>132.72</td>\n",
              "      <td>109.60</td>\n",
              "      <td>89.12</td>\n",
              "      <td>153.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>GUA</td>\n",
              "      <td>19</td>\n",
              "      <td>185.13</td>\n",
              "      <td>78.05</td>\n",
              "      <td>107.08</td>\n",
              "      <td>29.12</td>\n",
              "      <td>156.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>CYT</td>\n",
              "      <td>20</td>\n",
              "      <td>185.96</td>\n",
              "      <td>57.94</td>\n",
              "      <td>128.02</td>\n",
              "      <td>54.01</td>\n",
              "      <td>131.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>CYT</td>\n",
              "      <td>21</td>\n",
              "      <td>175.49</td>\n",
              "      <td>56.40</td>\n",
              "      <td>119.09</td>\n",
              "      <td>49.72</td>\n",
              "      <td>125.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>URA</td>\n",
              "      <td>22</td>\n",
              "      <td>197.50</td>\n",
              "      <td>96.86</td>\n",
              "      <td>100.65</td>\n",
              "      <td>58.52</td>\n",
              "      <td>138.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>GUA</td>\n",
              "      <td>23</td>\n",
              "      <td>165.81</td>\n",
              "      <td>42.09</td>\n",
              "      <td>123.71</td>\n",
              "      <td>39.32</td>\n",
              "      <td>126.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>CYT</td>\n",
              "      <td>24</td>\n",
              "      <td>185.14</td>\n",
              "      <td>105.97</td>\n",
              "      <td>79.17</td>\n",
              "      <td>27.88</td>\n",
              "      <td>157.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>URA</td>\n",
              "      <td>25</td>\n",
              "      <td>138.28</td>\n",
              "      <td>47.71</td>\n",
              "      <td>90.56</td>\n",
              "      <td>41.39</td>\n",
              "      <td>96.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1JO7</td>\n",
              "      <td>URA</td>\n",
              "      <td>26</td>\n",
              "      <td>122.45</td>\n",
              "      <td>43.12</td>\n",
              "      <td>79.33</td>\n",
              "      <td>53.24</td>\n",
              "      <td>69.21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id resname  resid  ...  sasa-Main-Chain  sasa-Non-polar  sasa-All-polar\n",
              "0   1A60     GUA      1  ...            60.23           31.95          102.20\n",
              "1   1A60     GUA      2  ...           115.09           38.57          133.43\n",
              "2   1A60     GUA      3  ...           128.68           52.70          134.89\n",
              "3   1A60     ADE      4  ...           130.26           57.02          129.43\n",
              "4   1A60     GUA      5  ...           116.62           37.60          166.72\n",
              "5   1A60     CYT      6  ...           117.22           52.77          127.45\n",
              "6   1A60     URA      7  ...           107.38           65.07          116.10\n",
              "7   1A60     CYT      8  ...           147.59           91.49          201.53\n",
              "8   1A60     ADE      9  ...           105.80           85.06          148.30\n",
              "9   1A60     ADE     10  ...           102.82           58.00          116.18\n",
              "10  1A60     CYT     11  ...           127.53           61.77           95.86\n",
              "11  1A60     URA     12  ...           128.05           49.96          146.98\n",
              "12  1A60     CYT     13  ...           119.56           51.25          124.05\n",
              "13  1A60     URA     14  ...           119.81           50.95          112.25\n",
              "14  1A60     CYT     15  ...           116.28           48.56          115.84\n",
              "15  1A60     CYT     16  ...           120.54           56.56          122.32\n",
              "16  1A60     CYT     17  ...           123.96           52.76          112.42\n",
              "17  1A60     CYT     18  ...           113.16           50.22          115.16\n",
              "18  1A60     CYT     19  ...           114.74           45.91          113.46\n",
              "19  1A60     CYT     20  ...           114.67           44.53           96.46\n",
              "20  1A60     CYT     21  ...            87.89           32.04          135.30\n",
              "21  1A60     URA     22  ...            76.60           72.85          152.99\n",
              "22  1A60     URA     23  ...            79.41           76.54          157.17\n",
              "23  1A60     URA     24  ...            72.20           57.94           63.02\n",
              "24  1A60     URA     25  ...           120.63           48.52          105.64\n",
              "25  1A60     CYT     26  ...           115.47           64.97           89.12\n",
              "26  1A60     CYT     27  ...           120.70           50.48          107.88\n",
              "27  1A60     GUA     28  ...           119.91           48.21          100.45\n",
              "28  1A60     ADE     29  ...           116.38           46.27          117.69\n",
              "29  1A60     GUA     30  ...           114.11           34.84          128.98\n",
              "30  1A60     GUA     31  ...           127.67           52.86          110.69\n",
              "31  1A60     GUA     32  ...           122.48           53.92          126.47\n",
              "32  1A60     URA     33  ...            90.74           71.35          190.45\n",
              "33  1A60     CYT     34  ...           103.75           59.95          210.77\n",
              "34  1A60     ADE     35  ...           105.68           49.80           87.60\n",
              "35  1A60     URA     36  ...           111.17           50.06           90.38\n",
              "36  1A60     CYT     37  ...            80.83           35.63           65.13\n",
              "37  1A60     GUA     38  ...            95.13           42.09          104.22\n",
              "38  1A60     GUA     39  ...           111.49           40.67          110.31\n",
              "39  1A60     ADE     40  ...           148.55           62.45          125.71\n",
              "40  1A60     ADE     41  ...           114.59           41.17          128.13\n",
              "41  1A60     CYT     42  ...           120.96           48.05          154.97\n",
              "42  1A60     CYT     43  ...           103.39           51.57          192.64\n",
              "43  1A60     ADE     44  ...           191.36          144.32          246.50\n",
              "44  1HWQ     GUA      1  ...           115.34           92.39          146.40\n",
              "45  1HWQ     GUA      2  ...           122.98           41.92          141.78\n",
              "46  1HWQ     URA      3  ...           118.80           51.46          114.21\n",
              "47  1HWQ     GUA      4  ...           123.15           42.97          130.50\n",
              "48  1HWQ     CYT      5  ...           124.57           36.87          135.09\n",
              "49  1HWQ     GUA      6  ...            99.29           22.20          131.77\n",
              "50  1HWQ     ADE      7  ...           122.16           55.49          102.38\n",
              "51  1HWQ     ADE      8  ...           117.78           36.48          132.47\n",
              "52  1HWQ     GUA      9  ...           110.91           33.61          138.49\n",
              "53  1HWQ     GUA     10  ...           116.71           42.83          133.14\n",
              "54  1HWQ     GUA     11  ...           128.88           46.34          126.32\n",
              "55  1HWQ     CYT     12  ...           120.47           39.51          120.27\n",
              "56  1HWQ     GUA     13  ...           104.73           29.38          166.66\n",
              "57  1HWQ     URA     14  ...           121.41           51.97          136.45\n",
              "58  1HWQ     CYT     15  ...           103.81           30.82           87.00\n",
              "59  1HWQ     GUA     16  ...           133.42           76.54          225.36\n",
              "60  1HWQ     URA     17  ...            66.40           11.54           69.54\n",
              "61  1HWQ     CYT     18  ...           114.54           60.86          153.51\n",
              "62  1HWQ     GUA     19  ...           103.51           39.14          100.88\n",
              "63  1HWQ     CYT     20  ...           124.92           53.20          117.66\n",
              "64  1HWQ     CYT     21  ...           118.21           50.62          116.71\n",
              "65  1HWQ     CYT     22  ...           122.96           44.69          116.12\n",
              "66  1HWQ     CYT     23  ...           119.81           46.72          147.98\n",
              "67  1HWQ     GUA     24  ...           110.20           40.77          150.48\n",
              "68  1HWQ     ADE     25  ...           110.95           65.30          101.33\n",
              "69  1HWQ     GUA     26  ...           122.61           40.28          135.27\n",
              "70  1HWQ     CYT     27  ...           123.18           52.50          116.38\n",
              "71  1HWQ     GUA     28  ...           111.51           43.35          139.70\n",
              "72  1HWQ     CYT     29  ...           121.51           51.99          120.68\n",
              "73  1HWQ     CYT     30  ...           179.68           94.54          188.16\n",
              "74  1JO7     ADE      1  ...            96.77           91.46          160.76\n",
              "75  1JO7     GUA      2  ...            78.70           47.85          103.08\n",
              "76  1JO7     URA      3  ...           112.62           52.89          116.51\n",
              "77  1JO7     ADE      4  ...           121.01           66.98          117.25\n",
              "78  1JO7     GUA      5  ...           126.33           52.54          138.60\n",
              "79  1JO7     ADE      6  ...           129.52           51.20          112.70\n",
              "80  1JO7     ADE      7  ...           139.30           58.21          133.40\n",
              "81  1JO7     ADE      8  ...           136.67           60.41          126.87\n",
              "82  1JO7     CYT      9  ...           119.65           54.85          124.46\n",
              "83  1JO7     ADE     10  ...           129.58           58.74          145.08\n",
              "84  1JO7     ADE     11  ...           131.60           72.61          139.46\n",
              "85  1JO7     GUA     12  ...           128.04           46.40          153.80\n",
              "86  1JO7     GUA     13  ...           113.00           42.22          149.72\n",
              "87  1JO7     CYT     14  ...           115.28           52.25          116.31\n",
              "88  1JO7     URA     15  ...            93.80           29.67           74.91\n",
              "89  1JO7     URA     16  ...           121.18           88.94          146.16\n",
              "90  1JO7     CYT     17  ...           101.38           33.93          168.60\n",
              "91  1JO7     GUA     18  ...           109.60           89.12          153.20\n",
              "92  1JO7     GUA     19  ...           107.08           29.12          156.01\n",
              "93  1JO7     CYT     20  ...           128.02           54.01          131.95\n",
              "94  1JO7     CYT     21  ...           119.09           49.72          125.77\n",
              "95  1JO7     URA     22  ...           100.65           58.52          138.98\n",
              "96  1JO7     GUA     23  ...           123.71           39.32          126.49\n",
              "97  1JO7     CYT     24  ...            79.17           27.88          157.26\n",
              "98  1JO7     URA     25  ...            90.56           41.39           96.89\n",
              "99  1JO7     URA     26  ...            79.33           53.24           69.21\n",
              "\n",
              "[100 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyE55JqoJ3U9",
        "colab_type": "text"
      },
      "source": [
        "**Model to predict sasa all atoms**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g28gWe00Cgy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define training features and target, split the dataset into train/test \n",
        "from sklearn.model_selection import train_test_split\n",
        "X=cs_all[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]\n",
        "#X=sasa[['sasa-Total-Side','sasa-Main-Chain', 'sasa-Non-polar', 'sasa-All-polar']] \n",
        "y=sasa['sasa-All-atoms'] \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq86iWXOFuRb",
        "colab_type": "code",
        "outputId": "1149e405-6840-4820-eb8c-ad1f140431bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#unoptimized random forest regression model train and evaluate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "sklearn_model = RandomForestRegressor(n_estimators=100)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred=sklearn_model.predict(X_test)\n",
        "# Evaluate it.\n",
        "print('R2 score for sasa all')\n",
        "print(sklearn_model.score(X_test, y_test))\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa all\n",
            "0.30307410285212233\n",
            "0.30307410285212233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur5c6u7GtmH6",
        "colab_type": "text"
      },
      "source": [
        "**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kf0r8b1WoXkB",
        "colab": {}
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5sq7X4ZLoXkG",
        "colab": {}
      },
      "source": [
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e97db88c-2309-45f5-f651-545869d2b1f5",
        "id": "dIsQ1FuNoXkI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator = sklearn_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  7.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=False,\n",
              "                                                   random_state=...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1991492f-2a1a-4a79-bad3-8e5547e86e8d",
        "id": "Rj2nrvj_onjP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_pred = rf_random.predict(X_test)\n",
        "print('R2 score for sasa all')\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa all\n",
            "0.31389272145268554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqTmiGCNKCWF",
        "colab_type": "text"
      },
      "source": [
        "**Model to Predict sasa total for side chains**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKroYeJwIt1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define training features and target, split the dataset into train/test \n",
        "from sklearn.model_selection import train_test_split\n",
        "X=cs_all[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]\n",
        "y=sasa['sasa-Total-Side'] \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d4841af5-e6b9-4b14-88af-64dfc03aed88",
        "id": "M2P_xIWRI-jr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#unoptimized random forest regression model train and evaluate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "sklearn_model = RandomForestRegressor(n_estimators=100)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred=sklearn_model.predict(X_test)\n",
        "# Evaluate it.\n",
        "print('R2 score for sasa sidechains')\n",
        "print(sklearn_model.score(X_test, y_test))\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa sidechains\n",
            "0.3437327330076304\n",
            "0.3437327330076304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "54hL9JQztsND"
      },
      "source": [
        "**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WGtfppjptsNG",
        "colab": {}
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7waSaXU1tsNJ",
        "colab": {}
      },
      "source": [
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "617b86d3-b717-4fd4-c4c1-f08cb6f685a4",
        "id": "xWyGR_8itsNL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator = sklearn_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  6.8min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=False,\n",
              "                                                   random_state=...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a25c5788-ed57-4c8f-a9c8-e481d2044df1",
        "id": "sNvpLscStsNO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_pred = rf_random.predict(X_test)\n",
        "print('R2 score')\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa all\n",
            "0.36059041417324167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbevxN9IKKdN",
        "colab_type": "text"
      },
      "source": [
        "**Model to predict Main Chain sasa**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FbhFqmKJH00",
        "colab": {}
      },
      "source": [
        "#define training features and target, split the dataset into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X=cs_all[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]\n",
        "y=sasa['sasa-Main-Chain'] \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d267059d-ce1b-4e7a-faf7-aac485ff4365",
        "id": "JOpu2O6KJH03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#unoptimized random forest regression model train and evaluate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "sklearn_model = RandomForestRegressor(n_estimators=100)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred=sklearn_model.predict(X_test)\n",
        "# Evaluate it.\n",
        "print('R2 score for sasa main chain')\n",
        "print(sklearn_model.score(X_test, y_test))\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa main chain\n",
            "0.20406666068279533\n",
            "0.20406666068279533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C55foLRptuDo"
      },
      "source": [
        "**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_oRhn7_StuDp",
        "colab": {}
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLIKEXcetuDt",
        "colab": {}
      },
      "source": [
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d59d52e3-2522-43e2-83ab-20f5c49c7cdd",
        "id": "MM6Ta-59tuDv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator = sklearn_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  8.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=False,\n",
              "                                                   random_state=...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6872f1d9-db46-4b7b-9039-66c250f35059",
        "id": "BED3QZ2btuDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_pred = rf_random.predict(X_test)\n",
        "print('R2 score')\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa all\n",
            "0.2220671461512954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiPr2hsrKROD",
        "colab_type": "text"
      },
      "source": [
        "**Model to predict non-polar sasa**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hheLEEz0Jb5g",
        "colab": {}
      },
      "source": [
        "#define training features and target, split the dataset into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X=cs_all[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]\n",
        "y=sasa['sasa-Non-polar'] \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ad43451c-4e3e-4d1d-bceb-767e9e329906",
        "id": "SJTHDSULJb5l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#unoptimized random forest regression model train and evaluate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "sklearn_model = RandomForestRegressor(n_estimators=100)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred=sklearn_model.predict(X_test)\n",
        "# Evaluate it.\n",
        "print('R2 score for sasa non-polar')\n",
        "print(sklearn_model.score(X_test, y_test))\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa non-polar\n",
            "0.36687104050879116\n",
            "0.36687104050879116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GfAHAo6JtvN_"
      },
      "source": [
        "**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rIOL8eaGtvOA",
        "colab": {}
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mNadCQp0tvOC",
        "colab": {}
      },
      "source": [
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bd2a0175-9349-45a6-8c11-95d73dc826fe",
        "id": "aCB5ZzrqtvOE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator = sklearn_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  7.4min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=False,\n",
              "                                                   random_state=...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea740efd-7158-4aac-a521-8d957bb937c6",
        "id": "4IieLh4_tvOG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_pred = rf_random.predict(X_test)\n",
        "print('R2 score')\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa all\n",
            "0.3837979365863692\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h6S1iFYKcHV",
        "colab_type": "text"
      },
      "source": [
        "**Model to predict sasa for all polar residues**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CC6w-ZWaJnmu",
        "colab": {}
      },
      "source": [
        "#define training features and target, split the dataset into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X=cs_all[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]]\n",
        "y=sasa['sasa-All-polar'] \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f48221f6-de22-46f6-a320-468d59ccfe66",
        "id": "_Sje54adJnmx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#unoptimized random forest regression model train and evaluate\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "sklearn_model = RandomForestRegressor(n_estimators=100)\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "y_pred=sklearn_model.predict(X_test)\n",
        "# Evaluate it.\n",
        "print('R2 score for sasa polar')\n",
        "print(sklearn_model.score(X_test, y_test))\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa polar\n",
            "0.2580327615467207\n",
            "0.2580327615467207\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aCmmEmQBtwmH"
      },
      "source": [
        "**Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2owQmXNTtwmI",
        "colab": {}
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lMWNWqVutwmK",
        "colab": {}
      },
      "source": [
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a021138a-a26c-4e88-96b3-ed2c142cf2ec",
        "id": "nP57SmzbtwmN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        }
      },
      "source": [
        "rf_random = RandomizedSearchCV(estimator = sklearn_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:  7.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=False,\n",
              "                                                   random_state=...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4],\n",
              "                                        'min_samples_split': [2, 5, 10],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "989296b9-90a4-4957-d762-7f45fd8beda1",
        "id": "CJrdJjD-twmP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y_pred = rf_random.predict(X_test)\n",
        "print('R2 score')\n",
        "print(r2_score(y_test,y_pred))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R2 score for sasa all\n",
            "0.2828191948173526\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}